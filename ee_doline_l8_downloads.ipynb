{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbd04f96-ddb8-419e-bae9-bb9887da4ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "import os\n",
    "import ee\n",
    "from ee import batch\n",
    "import time\n",
    "import requests\n",
    "import zipfile\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import rasterio as rio\n",
    "from rasterio.windows import from_bounds, Window\n",
    "from rasterio.warp import reproject\n",
    "from rasterio.enums import Resampling, ColorInterp\n",
    "import datetime\n",
    "import pandas as pd\n",
    "\n",
    "# Trigger the authentication flow.\n",
    "# ee.Authenticate()\n",
    "# Initialize the library.\n",
    "ee.Initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815e4a7d-95fb-4857-8363-302c8f45daee",
   "metadata": {},
   "source": [
    "# functions for pansharpening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffc9a970-522a-404e-be92-0a0be82cfdab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateRatio(rgb, pan, weight):\n",
    "    return pan / ((rgb[0] + rgb[1] + rgb[2] * weight) / (2 + weight))\n",
    "\n",
    "def Brovey(rgb, pan, weight, pan_dtype):\n",
    "    with np.errstate(invalid='ignore', divide='ignore'):\n",
    "        ratio = calculateRatio(rgb, pan, weight)\n",
    "    with np.errstate(invalid='ignore'):\n",
    "        sharp = np.clip(ratio * rgb, 0, np.iinfo(pan_dtype).max)\n",
    "        return sharp.astype(pan_dtype), ratio\n",
    "    \n",
    "def _upsample(rgb, panshape, src_aff, src_crs, to_aff, to_crs):\n",
    "    up_rgb = np.empty((rgb.shape[0], panshape[0],panshape[1]), dtype=rgb.dtype)\n",
    "    reproject(rgb, up_rgb,src_transform=src_aff,src_crs=src_crs,dst_transform=to_aff,\n",
    "              dst_crs=to_crs,resampling=Resampling.bilinear)\n",
    "    return up_rgb\n",
    "\n",
    "def _create_apply_mask(rgb):\n",
    "    color_mask = np.all(np.rollaxis(rgb, 0, 3) != 0,axis=2).astype(np.uint16) * np.iinfo(np.uint16).max\n",
    "    masked_rgb = np.array([np.minimum(band, color_mask) for band in rgb])\n",
    "    return masked_rgb\n",
    "\n",
    "def pansharpen(vis, vis_transform, pan, pan_transform, pan_dtype, r_crs, dst_crs, weight=0.2, method=\"Brovey\", src_nodata=0):\n",
    "    rgb = _upsample(_create_apply_mask(vis), pan.shape, vis_transform, r_crs, pan_transform, dst_crs)\n",
    "    # Main Pansharpening Processing\n",
    "    if method == \"Brovey\":\n",
    "        pansharp, _ = Brovey(rgb, pan, weight, pan_dtype)\n",
    "    # TODO: add other methods\n",
    "    return pansharp\n",
    "\n",
    "def add_years(d, years):\n",
    "    try:\n",
    "        return d.replace(year = d.year + years)\n",
    "    except ValueError:\n",
    "        if years < 0:\n",
    "            return d + (datetime.date(d.year + years, 3, 1) - datetime.date(d.year, 3, 1))\n",
    "        else:\n",
    "            return d + (datetime.date(d.year + years, 1, 1) - datetime.date(d.year, 1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6638c7b-50d8-4df5-977b-82aeb573b39c",
   "metadata": {},
   "source": [
    "# Landsat 8 Band info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0691fb8a-3f73-4e43-9ee1-c118bcfb8bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "band_info = {'B1': {'name':'Coastal aerosol','wavelength':'0.43 - 0.45 µm', 'resolution':'30 meters'}, \n",
    "             'B2': {'name':'Blue','wavelength':'0.45 - 0.51 µm', 'resolution':'30 meters'}, \n",
    "             'B3': {'name':'Green','wavelength':'0.53 - 0.59 µm', 'resolution':'30 meters'},\n",
    "             'B4': {'name':'Red','wavelength':'0.64 - 0.67 µm', 'resolution':'30 meters'},\n",
    "             'B5': {'name':'Near Infrared (NIR)','wavelength':'0.85 - 0.88 µm', 'resolution':'30 meters'},\n",
    "             'B6': {'name':'SWIR 1','wavelength':'1.57 - 1.65 µm', 'resolution':'30 meters'},\n",
    "             'B7': {'name':'SWIR 2','wavelength':'2.11 - 2.29 µm', 'resolution':'30 meters'},\n",
    "             'B8': {'name':'Panchromatic','wavelength':'0.52 - 0.90 µm', 'resolution':'30 meters'},\n",
    "             'B9': {'name':'Cirrus','wavelength':'1.36 - 1.38 µm', 'resolution':'30 meters'},\n",
    "             'B10': {'name':'Thermal Infrared (TIRS) 1','wavelength':'10.60 - 11.19 µm', 'resolution':'100 meters'},\n",
    "             'B11': {'name':'Thermal Infrared (TIRS) 2','wavelength':'11.50 - 12.51 µm', 'resolution':'100 meters'},\n",
    "             'BQA': {'name':'Quality Assessment','wavelength':'n/a', 'resolution':'n/a'}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2a22a2-0e2f-4ef6-94d7-aa750300f22a",
   "metadata": {},
   "source": [
    "# TODO: filter for clouds in region of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad4f5d7-7f8f-4131-bc70-ad3cdd83810f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TODO: filter for clouds in the region of interest using the below (currently just copy-pasted from Armin's code)\n",
    "\n",
    "# def cloudscore(image):\n",
    "#     '''\n",
    "#     Inner function for computing cloud score such that we can remove \n",
    "#     bad images from the landsat collections we download.\n",
    "#     Implementation in javascript can be found of Google Earth Engine \n",
    "#     website under (landsat algorithms), translation to python by KH.\n",
    "#     Further help from Nicholas Clinton at \n",
    "#     https://gis.stackexchange.com/questions/252685/filter-landsat-images-base-on-cloud-cover-over-a-region-of-interest\n",
    "#     '''\n",
    "#     cloud = ee.Algorithms.Landsat.simpleCloudScore(image).select('cloud')\n",
    "#     cloudiness = cloud.reduceRegion(ee.Reducer.mean(),\n",
    "#                                     geometry=region,\n",
    "#                                     scale=30)\n",
    "#     image = image.set(cloudiness)\n",
    "#     return image\n",
    "\n",
    "# # use this after filtering for the region of interest \n",
    "# cloud_tol = 15\n",
    "# collection = collection.map(algorithm=cloudscore).filter(ee.Filter.lt('cloud', cloud_tol))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7dddcc1-1613-40e9-b27d-19a2668f402c",
   "metadata": {},
   "source": [
    "# download all doline data and produce plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "014e0509-f9ed-4457-9bf8-a0efae76e8c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doline 001: Amery (67.64577, -72.34740), 2019-06-09 - 2019-06-11\n",
      "  downloaded scene (of 10) --> 1 2 3 4 5 6 7 8 9 10  \n",
      "  plotted scene (of 10) --> 1 2 3 4 5 6 7 8 9 10  \n",
      "Doline 002: Amery (71.03857, -71.74702), 2014-10-03 - 2014-10-18\n",
      "  downloaded scene (of 13) --> 1 2 3 4 5 6 7 8 9 10 11 12 13  \n",
      "  plotted scene (of 13) --> 1 2 3 4 5 6 7 8 9 10 11 12 13  \n",
      "Doline 003: George VI (-67.69910, -71.51163), 2016-01-12 - 2016-02-09\n",
      "  downloaded scene (of 18) --> 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  \n",
      "  plotted scene (of 18) --> 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  \n",
      "Doline 004: Riiser-Larsen (-11.16836, -72.06190), 2014-02-25 - 2014-09-19\n",
      "  downloaded scene (of 17) --> 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  \n",
      "  plotted scene (of 17) --> 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  \n",
      "Doline 005: George VI (-67.58669, -71.62512), 2015-03-17 - 2015-09-09\n",
      "  downloaded scene (of 16) --> 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  \n",
      "  plotted scene (of 16) --> 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  \n",
      "Doline 006: Amery (69.58815, -72.03700), 2015-03-28 - 2015-04-02\n",
      "  downloaded scene (of 11) --> 1 2 3 4 5 6 7 8 9 10 11  \n",
      "  plotted scene (of 11) --> 1 2 3 4 5 6 7 8 9 10 11  \n",
      "Doline 007: Amery (71.50130, -71.30820), 2019-03-30 - 2019-09-04\n",
      "  downloaded scene (of 12) --> 1 2 3 4 5 6 7 8 9 10 11 12  \n",
      "  plotted scene (of 12) --> 1 2 3 4 5 6 7 8 9 10 11 12  \n",
      "Doline 008: Amery (67.49296, -71.88709), 2014-10-18 - 2014-10-23\n",
      "  downloaded scene (of 10) --> 1 2 3 4 5 6 7 8 9 10  \n",
      "  plotted scene (of 10) --> 1 2 3 4 5 6 7 8 9 10  \n",
      "Doline 009: Amery (70.78402, -71.92557), 2020-04-02 - 2020-09-18\n",
      "  downloaded scene (of 13) --> 1 2 3 4 5 6 7 8 9 10 11 12 13  \n",
      "  plotted scene (of 13) --> 1 2 3 4 5 6 7 8 9 10 11 12 13  \n",
      "Doline 010: Amery (69.29339, -72.39613), 2019-07-24 - 2019-09-18\n",
      "  downloaded scene (of 13) --> 1 2 3 4 5 6 7 8 9 10 11 12 13  \n",
      "  plotted scene (of 13) --> 1 2 3 4 5 6 7 8 9 10 11 12 13  \n",
      "Doline 011: George VI (-67.80585, -70.99862), 2020-01-02 - 2020-03-23\n",
      "  downloaded scene (of 30) --> 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30  \n",
      "  plotted scene (of 30) --> 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30  \n",
      "Doline 012: Amery (69.51401, -72.34572), 2020-12-14 - 2020-12-21\n",
      "  downloaded scene (of 10) --> 1 2 3 4 5 6 7 8 9 10  \n",
      "  plotted scene (of 10) --> 1 2 3 4 5 6 7 8 9 10  \n",
      "Doline 013: Amery (72.74629, -70.47532), 2020-02-14 - 2020-02-21\n",
      "  downloaded scene (of 10) --> 1 2 3 4 5 6 7 8 9 10  \n",
      "  plotted scene (of 10) --> 1 2 3 4 5 6 7 8 9 10  \n",
      "Doline 014: George VI (-67.78925, -71.45909), 2018-02-07 - 2018-03-02\n",
      "  downloaded scene (of 16) --> 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  \n",
      "  plotted scene (of 16) --> 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  \n",
      "Doline 017: Shackleton (100.15366, -66.27061), 2017-12-20 - 2017-12-27\n",
      "  downloaded scene (of 10) --> 1 2 3 4 5 6 7 8 9 10  \n",
      "  plotted scene (of 10) --> 1 2 3 4 5 6 7 8 9 10  \n",
      "Doline 018: Amery (68.55895, -72.18430), 2020-04-02 - 2020-09-14\n",
      "  downloaded scene (of 11) --> 1 2 3 4 5 6 7 8 9 10 11  \n",
      "  plotted scene (of 11) --> 1 2 3 4 5 6 7 8 9 10 11  \n",
      "Doline 019: Amery (66.60989, -73.12284), 2014-03-17 - 2014-09-14\n",
      "  downloaded scene (of 11) --> 1 2 3 4 5 6 7 8 9 10 11  \n",
      "  plotted scene (of 11) --> 1 2 3 4 5 6 7 8 9 10 11  \n",
      "Doline 020: Amery (67.11467, -72.69375), 2017-03-14 - 2017-09-08\n",
      "  downloaded scene (of 16) --> 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  \n",
      "  plotted scene (of 16) --> 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  \n",
      "Doline 021: George VI (-67.62515, -71.52767), 2021-02-15 - 2021-03-10\n",
      "  downloaded scene (of 16) --> 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  \n",
      "  plotted scene (of 16) --> 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  \n",
      "Doline 023: Wilkins (-70.06164, -70.27910), 2020-01-24 - 2020-01-31\n",
      "  downloaded scene (of 10) --> 1 2 3 4 5 6 7 8 9 10  \n",
      "  plotted scene (of 10) --> 1 2 3 4 5 6 7 8 9 10  \n",
      "Doline 024: Wilkins (-70.62006, -70.25850), 2017-01-15 - 2017-02-25\n",
      "  downloaded scene (of 24) --> 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24  \n",
      "  plotted scene (of 24) --> 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24  \n",
      "Doline 027: Larsen C (-63.55850, -66.22238), 2016-04-18 - 2016-08-24\n",
      "  downloaded scene (of 16) --> 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  \n",
      "  plotted scene (of 16) --> 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  \n",
      "Doline 028: Pine Island (-99.57637, -74.47354), 2019-12-23 - 2019-12-25\n",
      "  downloaded scene (of 10) --> 1 2 3 4 5 6 7 8 9 10  \n",
      "  plotted scene (of 10) --> 1 2 3 4 5 6 7 8 9 10  \n",
      "Doline 030: Abbott (-101.29298, -72.23435), 2019-11-19 - 2019-12-28\n",
      "  downloaded scene (of 20) --> 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  \n",
      "  plotted scene (of 20) --> 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  \n"
     ]
    }
   ],
   "source": [
    "doline_data = 'doline_data/dolinesL8data_20211207.csv'\n",
    "n_scenes_before_after = 5\n",
    "buffer_around_center_meters = 4000\n",
    "equalize_bands = True # wether to plot all bands for each doline on same range of values\n",
    "redownload_scenes = False\n",
    "\n",
    "download_folder_zip = 'tmp_downloads_zip/'\n",
    "download_folder_scenes = 'ee_test_downloads/'\n",
    "\n",
    "doline_df = pd.read_csv(doline_data)\n",
    "for idx, doline_id_num in enumerate(doline_df.id):\n",
    "    doline_id = '%03d' % doline_id_num\n",
    "    lower_lim = doline_df.formation_lower.iloc[idx]\n",
    "    upper_lim = doline_df.formation_upper.iloc[idx]\n",
    "    center_lat = doline_df.lat.iloc[idx]\n",
    "    center_lon = doline_df.lon.iloc[idx]\n",
    "    shelf = doline_df.shelf.iloc[idx]\n",
    "\n",
    "    print('Doline %s: %s (%.5f, %.5f), %s - %s' % (doline_id,shelf,center_lon,center_lat,lower_lim,upper_lim))\n",
    "\n",
    "    dateformat = '%Y-%m-%d'\n",
    "    lower_time = datetime.datetime.strptime(lower_lim, dateformat)\n",
    "    upper_time = datetime.datetime.strptime(upper_lim, dateformat)\n",
    "    search_start = add_years(lower_time, years=-1).strftime(dateformat)\n",
    "    search_end = add_years(upper_time, years=1).strftime(dateformat)\n",
    "\n",
    "    center_lonlat = [center_lon,center_lat]\n",
    "    poi = ee.Geometry.Point(center_lonlat[0],center_lonlat[1])\n",
    "    roi = poi.buffer(buffer_around_center_meters)\n",
    "    # collection = ee.ImageCollection(\"LANDSAT/LC08/C02/T1_L2\") \\ # doesn't see to be avialable\n",
    "    collection = ee.ImageCollection('LANDSAT/LC08/C01/T2') \\\n",
    "        .filterDate(search_start, search_end) \\\n",
    "        .filterBounds(poi)\n",
    "    n_imgs = collection.size().getInfo()\n",
    "    # print('number of images: %d' % n_imgs)\n",
    "    info = collection.getInfo()\n",
    "\n",
    "    # get all the scenes to be downloaded (n_scenes_before_after around the estimated timeframe of formation)\n",
    "    times, timestrings, prodIDs, scenenames, crss = [], [], [], [], []\n",
    "    for i in range(n_imgs):\n",
    "        times.append(datetime.datetime.strptime(info['features'][i]['properties']['DATE_ACQUIRED'],dateformat))\n",
    "        timestrings.append(info['features'][i]['properties']['DATE_ACQUIRED'])\n",
    "        prodIDs.append(info['features'][i]['properties']['LANDSAT_PRODUCT_ID'])\n",
    "        scenenames.append(info['features'][i]['id'])\n",
    "        crss.append(info['features'][i]['bands'][1]['crs'])\n",
    "    timearray = np.array(times)\n",
    "    timedf = pd.DataFrame({'iScene': np.arange(n_imgs), \n",
    "                           'date': timearray,\n",
    "                           'product_id': prodIDs,\n",
    "                           'scene_name': scenenames,\n",
    "                           'crs': crss,\n",
    "                           'date_string': timestrings})\n",
    "    pd.options.mode.chained_assignment = None\n",
    "    timedf['between'] = (timedf.date > lower_time) & (timedf.date < upper_time)\n",
    "    timedf['before_lower'] = lower_time-timedf.date \n",
    "    timedf.loc[timedf['before_lower'] < datetime.timedelta(0), 'before_lower'] = datetime.timedelta(99999)\n",
    "    timedf['after_upper'] = timedf.date-upper_time\n",
    "    timedf.loc[timedf['after_upper'] < datetime.timedelta(0), 'after_upper'] = datetime.timedelta(99999)\n",
    "    timedf[['before','after']] = False\n",
    "    timedf.sort_values(by='after_upper',ascending=True,inplace=True)\n",
    "    timedf.iloc[:n_scenes_before_after]['after'] = True\n",
    "    timedf.sort_values(by='before_lower',ascending=True,inplace=True)\n",
    "    timedf.iloc[:n_scenes_before_after]['before'] = True\n",
    "    scenedf = timedf[(timedf.between | timedf.before | timedf.after)]\n",
    "    scenedf.drop(columns=['between', 'before', 'after', 'before_lower', 'after_upper'],inplace=True)\n",
    "    scenedf.sort_values('date', ascending=True, inplace=True)\n",
    "    scenedf.reset_index(drop=True,inplace=True)\n",
    "    \n",
    "    # download all the data\n",
    "    print('  downloaded scene (of %d)' % (len(scenedf.iScene)), end=' --> ')\n",
    "    for j, iScene in enumerate(scenedf.iScene):\n",
    "        try:\n",
    "            sceneName = scenedf.scene_name.iloc[j]\n",
    "            thisScene = ee.Image(sceneName)\n",
    "            fn = scenedf.product_id.iloc[j]\n",
    "            download_this_scene = True\n",
    "            if (not redownload_scenes) & os.path.exists(download_folder_scenes+fn+'.B2.tif'):\n",
    "                download_this_scene = False\n",
    "            if download_this_scene:\n",
    "                downloadURL = thisScene.getDownloadUrl({\n",
    "                                'name': fn,\n",
    "                                'bands':[{'id':'B1','scale':30},\n",
    "                                         {'id':'B2','scale':30},\n",
    "                                         {'id':'B3','scale':30},\n",
    "                                         {'id':'B4','scale':30},\n",
    "                                         {'id':'B5','scale':30},\n",
    "                                         {'id':'B6','scale':30},\n",
    "                                         {'id':'B7','scale':30},\n",
    "                                         {'id':'B8','scale':15},\n",
    "                                         {'id':'B9','scale':30},\n",
    "                                         {'id':'B10','scale':100},\n",
    "                                         {'id':'B11','scale':100}],\n",
    "                                'crs': scenedf.crs.iloc[j],\n",
    "                                'region': roi})\n",
    "\n",
    "                response = requests.get(downloadURL)\n",
    "\n",
    "                if not os.path.exists(download_folder_zip): os.makedirs(download_folder_zip)\n",
    "                with open(download_folder_zip+fn+'.zip', 'wb') as fd:\n",
    "                    fd.write(response.content)\n",
    "\n",
    "                with zipfile.ZipFile(download_folder_zip+fn+'.zip',\"r\") as zip_ref:\n",
    "                    zip_ref.extractall(download_folder_scenes)\n",
    "\n",
    "            print('%d'%(j+1),end=' ')\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            \n",
    "    print(' ')\n",
    "    \n",
    "    # get the max/min values across all scenes for each band\n",
    "    band_list = list(band_info.keys())[:-1]\n",
    "    data_mins_list = {k: [] for k in band_list}\n",
    "    data_maxs_list = {k: [] for k in band_list}\n",
    "    try:\n",
    "        for fn in scenedf.product_id:\n",
    "            for i in range(11):\n",
    "                im = rio.open('%s%s.B%d.tif' % (download_folder_scenes,fn,i+1))\n",
    "                imarray = im.read(1)\n",
    "                data_mins_list[band_list[i]].append(imarray[imarray>0].min())\n",
    "                data_maxs_list[band_list[i]].append(imarray[imarray>0].max())\n",
    "        data_mins_dict = {k: np.min(data_mins_list[k]) for k in band_list}\n",
    "        data_maxs_dict = {k: np.max(data_maxs_list[k]) for k in band_list}\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    \n",
    "    # make the plots\n",
    "    print('  plotted scene (of %d)' % (len(scenedf.iScene)), end=' --> ')\n",
    "    for j, fn in enumerate(scenedf.product_id):\n",
    "        try:\n",
    "            fig = plt.figure(figsize=[8,6],dpi=150)\n",
    "            for i in range(11):\n",
    "                ax = fig.add_subplot(3,4,i+1)\n",
    "                im = rio.open('%s%s.B%d.tif' % (download_folder_scenes,fn,i+1))\n",
    "                imarray = im.read(1)\n",
    "                imarray[imarray==0] = np.mean(imarray[imarray!=0])\n",
    "                vmin = data_mins_dict[band_list[i]]\n",
    "                vmax = data_maxs_dict[band_list[i]]\n",
    "                if equalize_bands:\n",
    "                    implot = ax.imshow(imarray, interpolation='nearest',cmap='gray',vmin=vmin,vmax=vmax)\n",
    "                else:\n",
    "                    thismin = np.mean(imarray)-3*np.std(imarray)\n",
    "                    thismax = np.mean(imarray)+3*np.std(imarray)\n",
    "                    implot = ax.imshow(imarray, interpolation='nearest',cmap='gray',vmin=thismin,vmax=thismax)\n",
    "                cbar = fig.colorbar(implot,ax=ax)\n",
    "                cbar.ax.tick_params(labelsize=3) \n",
    "                ax.axis('off')\n",
    "                band_id = list(band_info.keys())[i]\n",
    "                band_name = band_info[band_id]['name']\n",
    "                band_wl = band_info[band_id]['wavelength']\n",
    "                ax.set_title('%s: %s (%s)'%(band_id,band_name,band_wl),fontsize=3)\n",
    "\n",
    "            # this is for the pansharpened rgb composite\n",
    "            ax = fig.add_subplot(3,4,12)\n",
    "            r = rio.open('%s%s.B%d.tif' % (download_folder_scenes,fn,4))\n",
    "            g = rio.open('%s%s.B%d.tif' % (download_folder_scenes,fn,3))\n",
    "            b = rio.open('%s%s.B%d.tif' % (download_folder_scenes,fn,2))\n",
    "            p = rio.open('%s%s.B%d.tif' % (download_folder_scenes,fn,8))\n",
    "            pan = p.read(1)\n",
    "            vis = np.zeros(shape=(3,r.height,r.width))\n",
    "            for i,vband in enumerate([r,g,b]):\n",
    "                vis[i,:,:] = vband.read(1)\n",
    "            pan_transform = p.transform\n",
    "            vis_transform = r.transform\n",
    "            pansharpened = pansharpen(vis=vis, vis_transform=vis_transform, pan=pan, pan_transform=pan_transform,\n",
    "                                      pan_dtype=pan.dtype, r_crs=r.crs, dst_crs=p.crs)\n",
    "            data = []\n",
    "            for i,bandid in enumerate(['B4','B3','B2']):\n",
    "                imarray = pansharpened[i]\n",
    "                imarray[imarray==0] = np.mean(imarray[imarray!=0])\n",
    "                vmin = data_mins_dict[bandid]\n",
    "                vmax = data_maxs_dict[bandid]\n",
    "                if equalize_bands:\n",
    "                    imarray = np.uint8(np.clip(np.round((imarray-vmin) / (vmax-vmin) * 255),0,255))\n",
    "                else:\n",
    "                    # imarray = np.uint8(np.round((imarray-np.min(imarray)) / (np.max(imarray)-np.min(imarray)) * 255))\\\n",
    "                    thismin = np.mean(imarray)-3*np.std(imarray)\n",
    "                    thismin = np.min(imarray)\n",
    "                    thismax = np.mean(imarray)+3*np.std(imarray)\n",
    "                    imarray = np.uint8(np.clip(np.round((imarray-thismin) / (thismax-thismin) * 255),0,255))\n",
    "                data.append(imarray)\n",
    "            rgb = np.stack(data,axis=2)\n",
    "\n",
    "            implot = ax.imshow(rgb)\n",
    "            ax.axis('off')\n",
    "            ax.set_title('pansharpened true color',fontsize=4);\n",
    "\n",
    "            equal_title = '[bands equalized]' if equalize_bands else '[bands not equalized]'\n",
    "            fig.suptitle('Doline %s, %s, %s, (%.5f, %.5f), %s' % (doline_id, \n",
    "                                                                  shelf, \n",
    "                                                                  scenedf.date_string.iloc[j], \n",
    "                                                                  center_lon, \n",
    "                                                                  center_lat, \n",
    "                                                                  equal_title),fontsize=8)\n",
    "            fig.tight_layout()\n",
    "            fig_dir = 'figs/'\n",
    "            if not os.path.exists(fig_dir): os.makedirs(fig_dir)\n",
    "            equal_str = 'equalized' if equalize_bands else 'not-equalized'\n",
    "            figname = fig_dir + 'doline_%s_%s_%s_%s.jpg' % (doline_id,equal_str,scenedf.date_string.iloc[j],scenedf.product_id.iloc[j])\n",
    "            plt.savefig(figname, dpi=600)\n",
    "            plt.close()\n",
    "\n",
    "            print('%d'%(j+1),end=' ')\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1914a554-d476-4eb6-931f-0ab1c3def095",
   "metadata": {},
   "source": [
    "# ==========================================\n",
    "# for exporting figures to zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28d26e15-eb85-4a96-8124-9d1e6364ceff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zipdir(path, ziph):\n",
    "    # ziph is zipfile handle\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for file in files:\n",
    "            ziph.write(os.path.join(root, file), \n",
    "                       os.path.relpath(os.path.join(root, file), \n",
    "                                       os.path.join(path, '..')))\n",
    "\n",
    "zipfigfolder = 'zipped_figs/'\n",
    "if not os.path.exists(zipfigfolder): os.makedirs(zipfigfolder)\n",
    "zipf = zipfile.ZipFile(zipfigfolder+'figures_%s.zip'%datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M\"), 'w', zipfile.ZIP_DEFLATED)\n",
    "zipdir(fig_dir, zipf)\n",
    "zipf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f7c51e-c59b-4c31-a5be-164ce1e883bc",
   "metadata": {},
   "source": [
    "# =========================================="
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ee",
   "language": "python",
   "name": "ee"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
